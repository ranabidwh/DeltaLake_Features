{"cells":[{"cell_type":"code","source":["\n# MAGIC %md Description: Diving into the Delta Lake Features. This notebook covers Delta Lake Features using the Lending Club data.\n# MAGIC Convert your data from Parquet into Delta Lake Format\n# MAGIC Use Batch and Streaming as a sink source\n# MAGIC Use DML Operations for INSERT, UPDATE, DELETE OR MERGE\n# MAGIC Use SchemaMerge option\n# MAGIC Rever back to earliar version of table for TimeTravel\n# MAGIC Source Data for this notebook\n# MAGIC The data used is a modified version of the public data from Lending Club. It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"298d12a5-2714-4c2a-8bcd-5c23b1703282"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Setup\n# MAGIC To run this notebook, you have to create a cluster with version Databricks Runtime 7.6 or later"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19d69ad0-8ca8-4d06-8bb0-7a87f41533bf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["db = \"deltadb\"\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\nspark.sql(f\"USE {db}\")\n \nspark.sql(\"SET spark.databricks.delta.formatCheck.enabled = false\")\nspark.sql(\"SET spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec1c84c0-3f61-4d3b-b179-834faf7d6d05"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import random\nfrom datetime import datetime\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n \n \ndef my_checkpoint_dir(): \n  return \"/tmp/delta_demo/chkpt/%s\" % str(random.randint(0, 10000))\n \n# User-defined function to generate random state\n@udf(returnType=StringType())\ndef random_state():\n  return str(random.choice([\"CA\", \"TX\", \"NY\", \"WA\"]))\n \n \n# Function to start a streaming query with a stream of randomly generated load data and append to the parquet table\ndef generate_and_append_data_stream(table_format, table_name, schema_ok=False, type=\"batch\"):\n  \n  stream_data = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 500).load()\n    .withColumn(\"loan_id\", 10000 + col(\"value\"))\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\"))\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000))\n    .withColumn(\"addr_state\", random_state())\n    .withColumn(\"type\", lit(type)))\n    \n  if schema_ok:\n    stream_data = stream_data.select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"type\", \"timestamp\")\n      \n  query = (stream_data.writeStream\n    .format(table_format)\n    .option(\"checkpointLocation\", my_checkpoint_dir())\n    .trigger(processingTime = \"5 seconds\")\n    .table(table_name))\n \n  return query\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffc1fcfc-5ba5-4e20-bf87-2baaa01dc886"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function to stop all streaming queries \ndef stop_all_streams():\n    print(\"Stopping all streams\")\n    for s in spark.streams.active:\n        try:\n            s.stop()\n        except:\n            pass\n    print(\"Stopped all streams\")\n    dbutils.fs.rm(\"/tmp/delta_demo/chkpt/\", True)\n \n \ndef cleanup_paths_and_tables():\n    dbutils.fs.rm(\"/tmp/delta_demo/\", True)\n    dbutils.fs.rm(\"file:/dbfs/tmp/delta_demo/loans_parquet/\", True)\n        \n    for table in [\"deltadb.loans_parquet\", \"deltadb.loans_delta\", \"deltadb.loans_delta2\"]:\n        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n    \ncleanup_paths_and_tables()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abe3edf7-1f6a-4351-a784-d90b7dc61492"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Getting started with \n# MAGIC An open-source storage layer for data lakes that brings ACID transactions to Apache Spark™ and big data workloads.\n\n# MAGIC ACID Transactions: Ensures data integrity and read consistency with complex, concurrent data pipelines.\n# MAGIC Unified Batch and Streaming Source and Sink: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box.\n# MAGIC Schema Enforcement and Evolution: Ensures data cleanliness by blocking writes with unexpected.\n# MAGIC Time Travel: Query previous versions of the table by time or version number.\n# MAGIC Deletes and upserts: Supports deleting and upserting into tables with programmatic APIs.\n# MAGIC Open Format: Stored as Parquet format in blob storage.\n# MAGIC Audit History: History of all the operations that happened in the table.\n# MAGIC Scalable Metadata management: Able to handle millions of files are scaling the metadata operations with Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68021e49-2876-42d0-ab66-15e1d32e735f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Convert to Delta Lake format\n# MAGIC Delta Lake is 100% compatible with Apache Spark™, which makes it easy to get started with if you already use Spark for your big data workflows. Delta Lake features APIs for SQL, Python, and Scala, so that you can use it in whatever language you feel most comfortable in.\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac5e6a2a-ea98-47fc-bbd6-0df693a81c4d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md In Python: Read your data into a Spark DataFrame, then write it out in Delta Lake format directly, with no upfront schema definition needed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12e716c7-20c4-4530-a5ed-0ccf1ad81438"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["parquet_path = \"file:/dbfs/tmp/delta_demo/loans_parquet/\"\n \ndf = (spark.read.format(\"parquet\").load(parquet_path)\n      .withColumn(\"type\", lit(\"batch\"))\n      .withColumn(\"timestamp\", current_timestamp()))\n \ndf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"loans_delta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fc9bbee-9b6a-4f87-b30d-2da1a3e6084a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md SQL: Use CREATE TABLE statement with SQL (no upfront schema definition needed)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d69a3c66-9fc8-40ae-b829-e88c9b0eebf2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE TABLE loans_delta2\nUSING delta\nAS SELECT * FROM parquet.`/tmp/delta_demo/loans_parquet`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33dceb97-d07e-4f04-9004-22257f80d0eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#SQL: Use CONVERT TO DELTA to convert Parquet files to Delta Lake format in place\n%sql CONVERT TO DELTA parquet.`/tmp/delta_demo/loans_parquet`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9881c9e-66b9-4b76-9305-cae747cf15bf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md View the data in the Delta Lake table\n# MAGIC How many records are there, and what does the data look like?\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f7c5971-ece3-4a29-b2dc-d6706da3a8fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"select count(*) from loans_delta\"))\ndisplay(spark.sql(\"select * from loans_delta\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99b62e19-a72d-44c4-8591-4b79b76cbac4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Unified batch + streaming data processing with multiple concurrent readers and writers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea2691c8-18c1-4048-a85f-e82c415e5709"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Write 2 different data streams into our Delta Lake table at the same time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b1602a0-2eef-47ec-bb40-e92a5bc1f2ef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Set up 2 streaming writes to our table\nstream_query_A = generate_and_append_data_stream(table_format=\"delta\", table_name=\"loans_delta\", schema_ok=True, type='stream A')\nstream_query_B = generate_and_append_data_stream(table_format=\"delta\", table_name=\"loans_delta\", schema_ok=True, type='stream B')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec38439a-5b4b-476d-8c89-a2fc4fa22895"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Create 2 continuous streaming readers of our Delta Lake table to illustrate streaming progress.\n# Streaming read #1\ndisplay(spark.readStream.format(\"delta\").table(\"loans_delta\").groupBy(\"type\").count().orderBy(\"type\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91dea589-a858-45a5-aacc-96231c3e9a41"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Streaming read #2\ndisplay(spark.readStream.format(\"delta\").table(\"loans_delta\").groupBy(\"type\", window(\"timestamp\", \"10 seconds\")).count().orderBy(\"window\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"082aa365-058a-4969-b3e0-4d3680dc8b46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Add a batch query, just for good measure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd108a68-dd89-4e3d-99b7-d75ee4ef6d6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT addr_state, COUNT(*)\nFROM loans_delta\nGROUP BY addr_state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0af7288-b6af-4d59-baec-c2817ee2aacd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"stop\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6b4af1-3885-4866-b10b-8985cedc2546"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bfb17e6-c704-4454-997b-77f376b3b2ea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md ACID Transactions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bbb5799-71ec-43f0-8c16-cdc669da52c0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md View the Delta Lake transaction log"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e38695f-46e4-4a56-a719-e76bdc6b471d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql DESCRIBE HISTORY loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53dfbb38-b50b-4246-a336-7ce09cbb8082"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Use Schema Enforcement to protect data quality"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"255eeb94-eab8-433e-85c0-1a7c37c24b0c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md To show you how schema enforcement works, let's create a new table that has an extra column -- credit_score -- that doesn't match our existing Delta Lake table schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff24d13b-073d-4d11-9c7c-4d92a1d27bf7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Write DataFrame with extra column, credit_score, to Delta Lake table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8144e94-621b-4451-8618-c6025addb336"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Generate `new_data` with additional column\nnew_column = [StructField(\"credit_score\", IntegerType(), True)]\nnew_schema = StructType(spark.table(\"loans_delta\").schema.fields + new_column)\ndata = [(99997, 10000, 1338.55, \"CA\", \"batch\", datetime.now(), 649),\n        (99998, 20000, 1442.55, \"NY\", \"batch\", datetime.now(), 702)]\n \nnew_data = spark.createDataFrame(data, new_schema)\nnew_data.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"650d3c28-9ac7-4ece-a9c3-ed8c082e6f45"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Uncommenting this cell will lead to an error because the schemas don't match.\n# Attempt to write data with new column to Delta Lake table\nnew_data.write.format(\"delta\").mode(\"append\").saveAsTable(\"loans_delta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a40361c-e9bb-4d92-b09b-903b095c577f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Schema enforcement helps keep our tables clean and tidy so that we can trust the data we have stored in Delta Lake. The writes above were blocked because the schema of the new data did not match the schema of table (see the exception details). See more information about how it works here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6877dc43-e8e9-48c5-b295-e2219ce765ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Use Schema Evolution to add new columns to schema\n# MAGIC If we want to update our Delta Lake table to match this data source's schema, we can do so using schema evolution. Simply add the following to the Spark write command: .option(\"mergeSchema\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b41f8a6-f0bf-4341-ad18-f63fabab9856"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["new_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"loans_delta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55af8da7-bc90-4175-b598-95a86cf7677e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id IN (99997, 99998)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11694d68-36cc-433e-88a7-6266a52261fc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Delta Lake Time Travel\n# MAGIC Delta Lake’s time travel capabilities simplify building data pipelines for use cases including:\n\n# MAGIC %md Auditing Data Changes\n# MAGIC Reproducing experiments & reports\n# MAGIC %md Rollbacks\n# MAGIC As you write into a Delta table or directory, every operation is automatically versioned.\n# MAGIC\n# MAGIC \n# MAGIC \n# MAGIC You can query snapshots of your tables by:\n# MAGIC \n# MAGIC Version number, or\n# MAGIC Timestamp.\n# MAGIC using Python, Scala, and/or SQL syntax; for these examples we will use the SQL syntax.\n# MAGIC \n# MAGIC For more information, refer to the docs, or Introducing Delta Time Travel for Large Scale Data Lakes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e46acae1-05c9-4ada-b814-62b69ae54b0e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Review Delta Lake Table History for Auditing & Governance\n# MAGIC All the transactions for this table are stored within this table including the initial set of insertions, update, delete, merge, and inserts with schema modification"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2c1cd5a-d3ad-4c18-93ee-c73eaa8a60f7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"929667ca-d4b2-41da-bc1c-6e47790a3f2d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Use time travel to select and view the original version of our table (Version 0).\n# MAGIC %md As you can see, this version contains the original 14,705 records in it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00d3dc00-96d4-44df-a0e2-43aad88b61f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM loans_delta VERSION AS OF 0\").show(3)\nspark.sql(\"SELECT COUNT(*) FROM loans_delta VERSION AS OF 0\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64bf230f-6b1c-4c2e-b87a-ccce0e7fbd99"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql SELECT COUNT(*) FROM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eb26d98-af88-42d6-a8c6-7499996cab4f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Rollback a table to a specific version using RESTORE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c914ee5-1d18-4c8e-8fb9-ed029c182b48"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql RESTORE loans_delta VERSION AS OF 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fea620a8-3a5a-44ae-8b15-87619e826c66"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql SELECT COUNT(*) FROM loans_delta\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"938a8960-0311-44ea-8304-f1142e35b08a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md  Full DML Support: DELETE, UPDATE, MERGE INTO\n# MAGIC Delta Lake brings ACID transactions and full DML support to data lakes.\n\n# MAGIC Parquet does not support these commands - they are unique to Delta Lake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a24fd4c-a201-49c0-b75c-b88b7b146c86"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md DELETE: Handle GDPR or CCPA Requests on your Data Lake\n# MAGIC Imagine that we are responding to a GDPR data deletion request. The user with loan ID #4420 wants us to delete their data. Here's how easy it is."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcfe9340-3240-4bcb-9a26-b30fba0ac5bb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md View the user's data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbaa805d-25dd-461e-b0ca-22a2597c3193"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM loans_delta WHERE loan_id=4420\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"829a16b6-bf24-43a7-a88c-d1b553db139b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Delete the individual user's data with a single DELETE command using Delta Lake.\n\n# MAGIC %md Note: The DELETE command isn't supported in Parquet."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fe91245-d85d-4fd4-acb8-cdddbf2c2230"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDELETE FROM loans_delta WHERE loan_id=4420;\n-- Confirm the user's data was deleted\nSELECT * FROM loans_delta WHERE loan_id=4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd7d94b9-b7fe-44da-a7db-a89e92105fd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[" # MAGIC %md Use time travel and INSERT INTO to add the user back into our table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94c122d4-74d6-493b-92f9-fc20f6319139"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nINSERT INTO loans_delta\nSELECT * FROM loans_delta VERSION AS OF 0\nWHERE loan_id=4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4908fd6d-2e92-4e6f-9db8-3a12defa7d5d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id=4420\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70af1634-f967-40d6-8d1c-1d1dad636513"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md UPDATE: Modify the existing records in a table in one command\n%sql UPDATE loans_delta SET funded_amnt = 22000 WHERE loan_id = 4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"097eebc9-a9c5-4fc7-90a8-68677a4b33b2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id = 4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01ff057d-1f50-419f-9c21-0ef29a3a0331"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Support Change Data Capture Workflows & Other Ingest Use Cases via MERGE INTO\n# MAGIC With a legacy data pipeline, to insert or update a table, you must:\n# MAGIC \n# MAGIC Identify the new rows to be inserted\n# MAGIC Identify the rows that will be replaced (i.e. updated)\n# MAGIC Identify all of the rows that are not impacted by the insert or update\n# MAGIC Create a new temp based on all three insert statements\n# MAGIC Delete the original table (and all of those associated files)\n# MAGIC \"Rename\" the temp table back to the original table name\n# MAGIC Drop the temp table\n# MAGIC Merge process\n# MAGIC \n# MAGIC INSERT or UPDATE with Delta Lake\n# MAGIC 2-step process:\n# MAGIC \n# MAGIC Identify rows to insert or update\n# MAGIC Use MERGE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56fcfd27-eed3-4c2a-a526-ad9f642bcd86"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create merge table with 1 row update, 1 insertion\ndata = [(4420, 22000, 21500.00, \"NY\", \"update\", datetime.now()),  # record to update\n        (99999, 10000, 1338.55, \"CA\", \"insert\", datetime.now())]  # record to insert\nschema = spark.table(\"loans_delta\").schema\nspark.createDataFrame(data, schema).createOrReplaceTempView(\"merge_table\")\nspark.sql(\"SELECT * FROM merge_table\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adac6656-084c-405e-aded-1a4043f67031"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nMERGE INTO loans_delta AS l\nUSING merge_table AS m\nON l.loan_id = m.loan_id\nWHEN MATCHED THEN \n  UPDATE SET *\nWHEN NOT MATCHED \n  THEN INSERT *;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb755238-f932-4ae6-a6c7-1d3192c8b0e7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id IN (4420, 99999)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1b41329-103c-44dd-bfe9-ffa88dabc0ba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md File compaction and performance optimizations = faster queries\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10d586da-a2fc-44cd-a912-151662d52b37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Vacuum"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4afdfd7-c226-4c09-95fe-b438fb253185"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Vacuum deletes all files no longer needed by the current version of the table.\nVACUUM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f36ab0e2-6696-459f-bd80-f0124d8024ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# MAGIC %md Cache table in memory (Databricks Delta Lake only)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c44772d0-9cb7-459a-8df3-24dab64b9a7b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql CACHE SELECT * FROM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f36bd32-541e-4dde-996b-b4bfe1c1ef11"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[" # MAGIC %md Z-Order Optimize (Databricks Delta Lake only)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49076a08-d4ab-4a9a-988d-a634e4ed5bdf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql OPTIMIZE loans_delta ZORDER BY addr_state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42e57538-991c-49d6-9473-25f9dcc144e7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cleanup_paths_and_tables()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"274cb2a7-13b5-480d-beae-c9dbbce76a55"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Delta Lake Features","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4024940453039664}},"nbformat":4,"nbformat_minor":0}
